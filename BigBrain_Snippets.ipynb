{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "BigBrain Snippets.ipynb",
      "version": "0.3.2",
      "provenance": [],
      "collapsed_sections": [],
      "toc_visible": true,
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/debbysonino/LamasDataHack/blob/master/BigBrain_Snippets.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "TyFZFP2yN3vh"
      },
      "source": [
        "##BigBrain: Importing DS libraries\n",
        "These are libraries we will usually use when working with data sets"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "MFPrbLmqNDis",
        "colab": {}
      },
      "source": [
        "# General DS libraries we are going to need\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "eTZpu6-mNG9j"
      },
      "source": [
        "##BigBrain: Loading data from a csv\n",
        "This is how we load data into a csv using pandas, using our machine's disk to speed up future loads."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5HGlaJTEAYJu",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import os\n",
        "import pandas as pd\n",
        "\n",
        "# We define the datasets we want to load\n",
        "datasets = ('accounts', )\n",
        "source_prefix = 'https://storage.googleapis.com/mondaycom-datahack/final_sets/'\n",
        "\n",
        "local_dir = './datasets/datahack/'\n",
        "file_prefix = 'train_'\n",
        "file_suffix = ''\n",
        "file_extension = 'csv'\n",
        "\n",
        "# We create a directory for the datasets if it doesn't exist\n",
        "if not os.path.exists(local_dir):\n",
        "    os.makedirs(local_dir)\n",
        "\n",
        "# For each dataset we want, we check if we already downloaded it and fix it if we didn't\n",
        "for dataset in datasets:\n",
        "  if not os.path.isfile('{}{}{}{}.{}'.format(local_dir, file_prefix, dataset, file_suffix, file_extension)):\n",
        "    !curl {source_prefix}{file_prefix}{dataset}{file_suffix}.{file_extension} --output {local_dir}{file_prefix}{dataset}{file_suffix}.{file_extension}\n",
        "\n",
        "  # Load the datasets into a DataFrame using pandas\n",
        "  globals()[dataset] = pd.read_csv('{}{}{}{}.{}'.format(local_dir, file_prefix, dataset, file_suffix, file_extension), low_memory=False)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gMiOSPvyiMv1",
        "colab_type": "text"
      },
      "source": [
        "##BigBrain: Connecting Colab to remote machine\n",
        "If your machine isn't strong enough, and you wish to connect colab to a remote machine, you can run this command in a terminal and then tell colab to connect to your local machine.\n",
        "\n",
        "This command tunnels jupyter's 8888 port from your remote machine to your local machine, which basically means - when colab tries to connect to your local machine, it actually connects to the remote one."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "E33xEcCfiL6G",
        "colab": {}
      },
      "source": [
        "### Generic approach ###\n",
        "!ssh username@remote_machine_address -L 8888:localhost:8888\n",
        "\n",
        "### GCP Machines ###\n",
        "!gcloud compute ssh --zone your-region machine-name -- -L 8888:localhost:8888"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3fz41-yb1g5C",
        "colab_type": "text"
      },
      "source": [
        "##BigBrain: Submitting results to DataHack2019\n",
        "This is how to submit your predictions to data-hack's leaderboard"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sMu28ynv1trk",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Importing stuff for http requests\n",
        "from urllib import request\n",
        "import json\n",
        "\n",
        "# We validate first that we actually send all the test accounts expected to be sent\n",
        "if y_pred_submission.shape[0] != 71683 or submission_account_ids.shape[0] != 71683:\n",
        "  raise Exception(\"You have to send all of the accounts! Expected: (71683, 71683), Got: ({}, {})\".format(y_pred_submission.shape[0], submission_account_ids.shape[0]))\n",
        "\n",
        "if \"group_name\" not in vars() or group_name == \"\":\n",
        "  group_name = input(\"Please enter your group's name:\")\n",
        "\n",
        "data = json.dumps({'submitter': group_name, 'predictions': predictions}).encode('utf-8')\n",
        "\n",
        "req = request.Request(f\"https://leaderboard.datahack.org.il/monday/api/\",\n",
        "                      headers={'Content-Type': 'application/json'},\n",
        "                      data=data)\n",
        "\n",
        "res = request.urlopen(req)\n",
        "print(json.load(res))"
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}