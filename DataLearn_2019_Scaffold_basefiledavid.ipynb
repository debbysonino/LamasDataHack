{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "DataLearn 2019 - Scaffold.ipynb",
      "version": "0.3.2",
      "provenance": [],
      "collapsed_sections": [],
      "toc_visible": true,
      "machine_shape": "hm",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/debbysonino/LamasDataHack/blob/master/DataLearn_2019_Scaffold_basefiledavid.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dTnHVMK-wyjz",
        "colab_type": "text"
      },
      "source": [
        "&nbsp; ![alt text](https://s3.amazonaws.com/monday.com/static/svg/monday-logos/monday-footer-logo.svg)\n",
        "\n",
        "#Model scaffold\n",
        "This notebook is intended to get you up and running faster.\n",
        "\n",
        "It has the basic scaffold of an ML model, including:\n",
        "* Data loading\n",
        "* Feature extraction\n",
        "* Columns transformation\n",
        "* Training\n",
        "* Evaluating\n",
        "* Submitting results"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jhDZdkDTjyf-",
        "colab_type": "text"
      },
      "source": [
        "###Getting our depnedncies (and data!)\n",
        "First we'll import our relevant libraries"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Gao6_ydqMgBS",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# General DS libraries we are going to need\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "from datetime import timedelta\n",
        "\n",
        "# Importing our base model\n",
        "# [REDACTED ML MODEL USED]\n",
        "\n",
        "# Imports for working with our large dataset\n",
        "from sklearn.utils.random import sample_without_replacement\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "# We need those for data manipulation and getting our features ready for the model\n",
        "from sklearn.preprocessing import OneHotEncoder, Normalizer, Binarizer\n",
        "from sklearn.compose import make_column_transformer\n",
        "\n",
        "# These can be used to measure our model's performance\n",
        "from sklearn import metrics\n",
        "from sklearn.metrics import classification_report, confusion_matrix\n",
        "\n",
        "# Ignore DataFrame assignment warnings\n",
        "pd.options.mode.chained_assignment = None"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "jNoDkmNAsCaY"
      },
      "source": [
        "\n",
        "We set a few constants to use later on for sampeling and running the model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "JhqmBDsksCaa",
        "cellView": "form",
        "colab": {}
      },
      "source": [
        "#@title Model parameters { run: \"auto\" }\n",
        "# n_neighbors = 7 #@param {type:\"slider\", min:1, max:30, step:1}\n",
        "group_name = \"Lamassim\" #@param {type:\"string\"}\n",
        "samples_num = 220000 #@param {type:\"slider\", min:0, max:1500000, step:10000}\n",
        "n_jobs = -1 #@param {type:\"slider\", min:-1, max:32, step:1}\n",
        "path_prefix = \"https://storage.googleapis.com/mondaycom-datahack/final_sets\" #@param [\"https://storage.googleapis.com/mondaycom-datahack/final_sets\", \"https://mondaycom-datahack.s3.amazonaws.com/final_sets\"] {allow-input: true}"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "DpkqHB9lsUmQ"
      },
      "source": [
        "Next we'll load all the different parts of our dataset\n",
        "\n",
        "<br/>\n",
        "\n",
        "_Our use my data loading [snippet](https://colab.research.google.com/drive/1_Y-sZ5eHIDlDUMuLCwfnbuJdLh0DTXmO#scrollTo=5HGlaJTEAYJu&line=23&uniqifier=1)!_"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "b2x0lOgBZRP_",
        "colab_type": "code",
        "outputId": "c3b8af11-a19b-4296-a46f-e68e1bb847bb",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 221
        }
      },
      "source": [
        "import os\n",
        "import pandas as pd\n",
        "\n",
        "# We define the datasets we want to load\n",
        "datasets = ('accounts', 'users', 'events', 'subscriptions')\n",
        "source_prefix = 'https://storage.googleapis.com/mondaycom-datahack/final_sets/'\n",
        "\n",
        "local_dir = './datasets/datahack/'\n",
        "file_prefix = 'train_'\n",
        "file_suffix = ''\n",
        "file_extension = 'csv'\n",
        "\n",
        "# We create a directory for the datasets if it doesn't exist\n",
        "if not os.path.exists(local_dir):\n",
        "    os.makedirs(local_dir)\n",
        "\n",
        "# For each dataset we want, we check if we already downloaded it and fix it if we didn't\n",
        "for dataset in datasets:\n",
        "  if not os.path.isfile('{}{}{}{}.{}'.format(local_dir, file_prefix, dataset, file_suffix, file_extension)):\n",
        "    !curl {source_prefix}{file_prefix}{dataset}{file_suffix}.{file_extension} --output {local_dir}{file_prefix}{dataset}{file_suffix}.{file_extension}\n",
        "\n",
        "  # Load the datasets into a DataFrame using pandas\n",
        "  globals()['{}{}'.format(file_prefix, dataset)] = pd.read_csv('{}{}{}{}.{}'.format(local_dir, file_prefix, dataset, file_suffix, file_extension), low_memory=False)"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "  % Total    % Received % Xferd  Average Speed   Time    Time     Time  Current\n",
            "                                 Dload  Upload   Total   Spent    Left  Speed\n",
            "100  204M  100  204M    0     0   109M      0  0:00:01  0:00:01 --:--:--  109M\n",
            "  % Total    % Received % Xferd  Average Speed   Time    Time     Time  Current\n",
            "                                 Dload  Upload   Total   Spent    Left  Speed\n",
            "100  330M  100  330M    0     0  91.8M      0  0:00:03  0:00:03 --:--:-- 91.8M\n",
            "  % Total    % Received % Xferd  Average Speed   Time    Time     Time  Current\n",
            "                                 Dload  Upload   Total   Spent    Left  Speed\n",
            "100 1215M  100 1215M    0     0  82.8M      0  0:00:14  0:00:14 --:--:-- 85.2M\n",
            "  % Total    % Received % Xferd  Average Speed   Time    Time     Time  Current\n",
            "                                 Dload  Upload   Total   Spent    Left  Speed\n",
            "100 17.1M  100 17.1M    0     0  20.8M      0 --:--:-- --:--:-- --:--:-- 20.8M\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KdEFqnpfKjmk",
        "colab_type": "code",
        "outputId": "3bab0974-7f3e-49f7-a13c-44d6686777d2",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 221
        }
      },
      "source": [
        "import os\n",
        "import pandas as pd\n",
        "\n",
        "# We define the datasets we want to load\n",
        "datasets = ('accounts', 'users', 'events', 'subscriptions')\n",
        "source_prefix = 'https://storage.googleapis.com/mondaycom-datahack/final_sets/'\n",
        "\n",
        "local_dir = './datasets/datahack/'\n",
        "file_prefix = 'test_'\n",
        "file_suffix = ''\n",
        "file_extension = 'csv'\n",
        "\n",
        "# We create a directory for the datasets if it doesn't exist\n",
        "if not os.path.exists(local_dir):\n",
        "    os.makedirs(local_dir)\n",
        "\n",
        "# For each dataset we want, we check if we already downloaded it and fix it if we didn't\n",
        "for dataset in datasets:\n",
        "  if not os.path.isfile('{}{}{}{}.{}'.format(local_dir, file_prefix, dataset, file_suffix, file_extension)):\n",
        "    !curl {source_prefix}{file_prefix}{dataset}{file_suffix}.{file_extension} --output {local_dir}{file_prefix}{dataset}{file_suffix}.{file_extension}\n",
        "\n",
        "  # Load the datasets into a DataFrame using pandas\n",
        "  globals()['{}{}'.format(file_prefix, dataset)] = pd.read_csv('{}{}{}{}.{}'.format(local_dir, file_prefix, dataset, file_suffix, file_extension), low_memory=False)"
      ],
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "  % Total    % Received % Xferd  Average Speed   Time    Time     Time  Current\n",
            "                                 Dload  Upload   Total   Spent    Left  Speed\n",
            "100 10.6M  100 10.6M    0     0  7823k      0  0:00:01  0:00:01 --:--:-- 7823k\n",
            "  % Total    % Received % Xferd  Average Speed   Time    Time     Time  Current\n",
            "                                 Dload  Upload   Total   Spent    Left  Speed\n",
            "100 17.1M  100 17.1M    0     0  14.4M      0  0:00:01  0:00:01 --:--:-- 14.4M\n",
            "  % Total    % Received % Xferd  Average Speed   Time    Time     Time  Current\n",
            "                                 Dload  Upload   Total   Spent    Left  Speed\n",
            "100 63.2M  100 63.2M    0     0  56.8M      0  0:00:01  0:00:01 --:--:-- 56.8M\n",
            "  % Total    % Received % Xferd  Average Speed   Time    Time     Time  Current\n",
            "                                 Dload  Upload   Total   Spent    Left  Speed\n",
            "100  959k  100  959k    0     0  1191k      0 --:--:-- --:--:-- --:--:-- 1190k\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yh1wGtOsUcnf",
        "colab_type": "text"
      },
      "source": [
        "We need to add our test sets to our train sets and work on both at the same time.\n",
        "\n",
        "We'll split them back up before training and inference."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JNcD_YSBUbiY",
        "colab_type": "code",
        "outputId": "f203a9d8-a120-4a31-aa53-c0f93c502278",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 187
        }
      },
      "source": [
        "accounts = train_accounts.append(test_accounts)\n",
        "users = train_users.append(test_users)\n",
        "events = train_events.append(test_events)\n",
        "subscriptions = train_subscriptions.append(test_subscriptions)"
      ],
      "execution_count": 91,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/pandas/core/frame.py:6692: FutureWarning:\n",
            "\n",
            "Sorting because non-concatenation axis is not aligned. A future version\n",
            "of pandas will change to not sort by default.\n",
            "\n",
            "To accept the future behavior, pass 'sort=False'.\n",
            "\n",
            "To retain the current behavior and silence the warning, pass 'sort=True'.\n",
            "\n",
            "\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Jo5vLhgAlY-K",
        "colab_type": "text"
      },
      "source": [
        "###Feature engineering\n",
        "In this block we add a new feature of `[REDACTED]` extracted from the user `[REDACTED]`\n",
        "\n",
        "We also seperate all the `[REDACTED]` users into a different DataFrame"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ou0AYsbL_406",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "users['[REDACTED]'] = users['REDACTED'].apply(lambda x: \"[REDACTED]\")\n",
        "[REDACTED] = users[users[\"[REDACTED]\"] == \"[REDACTED]\"]"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "w6VUhsrzlvES",
        "colab_type": "text"
      },
      "source": [
        "Let's enrich our data a bit"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0f9X2owrCpl0",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Joining the accounts with the [REDACTED] users\n",
        "all_features = accounts.merge([REDACTED], on='account_id', suffixes=('_account', '_user'))\n",
        "\n",
        "all_features = all_features.reset_index().set_index(['account_id', '[REDACTED]']).drop(columns='index')\n",
        "\n",
        "# Adding the [REDACTED] in a seperate column\n",
        "all_features['[REDACTED]'] = \"[REDACTED]\""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-u2yzKUsKqsa",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "all_features=accounts"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "moN6itpxQAGi",
        "colab_type": "code",
        "outputId": "8716702e-1e32-406a-d4e1-b9fa6cfb4d65",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "all_features[all_features['lead_score'].isna()].shape[0]"
      ],
      "execution_count": 89,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "71683"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 89
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SngUmaPImHZF",
        "colab_type": "text"
      },
      "source": [
        "###Data preperation\n",
        "After we created our raw features we need to make sure the fit the way our ML model expects to receive them."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "z-KJGAevD0iT",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# We map our features into different types\n",
        "categorical_features = ['country', 'device']\n",
        "\n",
        "normalized_features = ['collection_21_days']\n",
        "\n",
        "binary_features = ['paying', 'has_logo']\n",
        "\n",
        "untouched_features = ['account_id']\n",
        "\n",
        "target = ['lead_score']\n",
        "\n",
        "# And create a column transformer to handle the manipulation for us\n",
        "preprocess = make_column_transformer(\n",
        "    (OneHotEncoder(), categorical_features),\n",
        "    (Normalizer(), normalized_features),\n",
        "    (Binarizer(), binary_features)\n",
        ")"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qMEWXPofVXQp",
        "colab_type": "text"
      },
      "source": [
        "###Re-splitting\n",
        "We now need to split our data back to the original train set and test set.\n",
        "\n",
        "We also make sure we keep only the columns we want in the data frame (the features)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "u2u-ghg6VsuF",
        "colab_type": "code",
        "outputId": "14d0d928-33a5-414d-f024-91539eb42dde",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 153
        }
      },
      "source": [
        "# Getting only the relevant features from the dataset\n",
        "dataset = all_features[categorical_features + normalized_features + binary_features + untouched_features + target]\n",
        "\n",
        "# Filling empty values with default values \n",
        "dataset.loc[:,categorical_features] = dataset[categorical_features].fillna('')\n",
        "dataset.loc[:,normalized_features +\n",
        "              binary_features +\n",
        "              untouched_features] = dataset[normalized_features +\n",
        "                                            binary_features +\n",
        "                                            untouched_features].fillna(0)\n",
        "\n",
        "# Splitting them back up to the original train/test split\n",
        "dataset_train = dataset[dataset.reset_index().account_id.isin(train_accounts.account_id)]\n",
        "dataset_test = dataset[dataset.reset_index().account_id.isin(test_accounts.account_id)]"
      ],
      "execution_count": 96,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:12: UserWarning:\n",
            "\n",
            "Boolean Series key will be reindexed to match DataFrame index.\n",
            "\n",
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:13: UserWarning:\n",
            "\n",
            "Boolean Series key will be reindexed to match DataFrame index.\n",
            "\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "L3F7pS0pT7Wb",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Splitting them back up to the original train/test split\n",
        "dataset_train = dataset.loc[~(dataset['lead_score'].isna())].reset_index().drop(['index'],axis=1)\n",
        "dataset_test = dataset.loc[(dataset['lead_score'].isna())].reset_index().drop(['index'],axis=1)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IM1ooT2BQtTE",
        "colab_type": "code",
        "outputId": "9840c239-3587-43cb-e573-f2ca56166603",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "dataset_test[dataset_test['lead_score'].isna()].shape,dataset[dataset['lead_score'].isna()].shape"
      ],
      "execution_count": 105,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "((71683, 7), (71683, 7))"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 105
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DeYB9q5Hmdtl",
        "colab_type": "text"
      },
      "source": [
        "###Setting everything up\n",
        "Our dataset is large (1,500,000+ accounts, each has a few users, each has events for every day)\n",
        "\n",
        "We need to work on a smaller batch of the training data so we can iterate more quickly.\n",
        "\n",
        "Once we find a good architecture we can increase the sample size to increase the accuracy."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9k_FH7UTUUwD",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 204
        },
        "outputId": "d49fa008-9ad0-4494-eb80-857370fc99f2"
      },
      "source": [
        ""
      ],
      "execution_count": 103,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>country</th>\n",
              "      <th>device</th>\n",
              "      <th>collection_21_days</th>\n",
              "      <th>paying</th>\n",
              "      <th>has_logo</th>\n",
              "      <th>account_id</th>\n",
              "      <th>lead_score</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>IL</td>\n",
              "      <td>desktop</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>7.0</td>\n",
              "      <td>NaN</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>MX</td>\n",
              "      <td>desktop</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>44.0</td>\n",
              "      <td>NaN</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>FJ</td>\n",
              "      <td>desktop</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>45.0</td>\n",
              "      <td>NaN</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>US</td>\n",
              "      <td>desktop</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>46.0</td>\n",
              "      <td>NaN</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>US</td>\n",
              "      <td>desktop</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>53.0</td>\n",
              "      <td>NaN</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "  country   device  collection_21_days  ...  has_logo  account_id  lead_score\n",
              "0      IL  desktop                   0  ...         1         7.0         NaN\n",
              "1      MX  desktop                   0  ...         1        44.0         NaN\n",
              "2      FJ  desktop                   0  ...         1        45.0         NaN\n",
              "3      US  desktop                   0  ...         1        46.0         NaN\n",
              "4      US  desktop                   0  ...         1        53.0         NaN\n",
              "\n",
              "[5 rows x 7 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 103
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Kt2uv6bbVVom",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "sampled_dataset_train = dataset_train.iloc[sample_without_replacement(dataset_train.shape[0], samples_num)]"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "V1pKJz2rDDLz",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# We fit our column transformer on both the train and the test sets\n",
        "preprocess.fit(sampled_dataset_train.append(dataset_test))\n",
        "\n",
        "# We use transform to finally manipulate the features of our training set\n",
        "X = preprocess.transform(sampled_dataset_train)\n",
        "\n",
        "# Seperating the label\n",
        "y = sampled_dataset_train.pop('lead_score')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2oOxDhLgM8a4",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# You now need to split the data into YOUR OWN training and test sets\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.05, random_state=42)\n",
        "\n",
        "# For standardization purposes we store y_test in a y_true variable\n",
        "y_true = y_test"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QDpBfat1IHbN",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 51
        },
        "outputId": "42ee657f-f191-4a74-f35a-420c00b9e0b6"
      },
      "source": [
        "X"
      ],
      "execution_count": 57,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<220000x218 sparse matrix of type '<class 'numpy.float64'>'\n",
              "\twith 668776 stored elements in Compressed Sparse Row format>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 57
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "h49yMjV2HMow",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "## (when using google colab)\n",
        "! pip install catboost\n",
        "! pip install plotly_express"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "khj17scYHQnH",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "## General\n",
        "import os \n",
        "import joblib\n",
        "import requests\n",
        "from google_drive_downloader import GoogleDriveDownloader as gdd\n",
        "\n",
        "## Data manipulation\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "\n",
        "## Modeling\n",
        "### Modeling pipeline\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.model_selection import cross_val_score\n",
        "from sklearn.metrics import classification_report, confusion_matrix, accuracy_score, precision_score, recall_score, f1_score\n",
        "from sklearn.model_selection import GridSearchCV\n",
        " \n",
        "### Models\n",
        "from sklearn.tree import DecisionTreeClassifier\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "from xgboost import XGBClassifier    \n",
        "from catboost import CatBoostClassifier, Pool\n",
        "\n",
        "## Visuatlization\n",
        "import plotly \n",
        "import plotly.express as px\n",
        "import plotly.graph_objects as go"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hZjlP5jFOuuZ",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 119
        },
        "outputId": "634ba04a-787b-4203-bc7b-bf823654d71e"
      },
      "source": [
        "y_train.tail()"
      ],
      "execution_count": 76,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "1264930    0.0\n",
              "520001     0.0\n",
              "725600     0.0\n",
              "67356      NaN\n",
              "770403     0.0\n",
              "Name: lead_score, dtype: float64"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 76
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lU_fiOvFHE0p",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 357
        },
        "outputId": "63f96ab2-a4b1-4b2f-90b0-1216484240d0"
      },
      "source": [
        "parameters = {'n_estimators': [50], #[50,100,200]\n",
        "             'max_depth': [5], #[5,10,15],\n",
        "             'criterion': ['gini', 'entropy'],\n",
        "             'max_features': [10]} #[int(np.log2(X_train.shape[1])), int(np.sqrt(X_train.shape[1]))]\n",
        "\n",
        "rf_cv = GridSearchCV(RandomForestClassifier(class_weight='balanced'), \n",
        "                   parameters, \n",
        "                   n_jobs = -1,\n",
        "                   cv = 5,\n",
        "                   refit = True,\n",
        "                   scoring = 'f1')\n",
        "\n",
        "rf_cv.fit(X_train, y_train)"
      ],
      "execution_count": 109,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "GridSearchCV(cv=5, error_score='raise-deprecating',\n",
              "             estimator=RandomForestClassifier(bootstrap=True,\n",
              "                                              class_weight='balanced',\n",
              "                                              criterion='gini', max_depth=None,\n",
              "                                              max_features='auto',\n",
              "                                              max_leaf_nodes=None,\n",
              "                                              min_impurity_decrease=0.0,\n",
              "                                              min_impurity_split=None,\n",
              "                                              min_samples_leaf=1,\n",
              "                                              min_samples_split=2,\n",
              "                                              min_weight_fraction_leaf=0.0,\n",
              "                                              n_estimators='warn', n_jobs=None,\n",
              "                                              oob_score=False,\n",
              "                                              random_state=None, verbose=0,\n",
              "                                              warm_start=False),\n",
              "             iid='warn', n_jobs=-1,\n",
              "             param_grid={'criterion': ['gini', 'entropy'], 'max_depth': [5],\n",
              "                         'max_features': [10], 'n_estimators': [50]},\n",
              "             pre_dispatch='2*n_jobs', refit=True, return_train_score=False,\n",
              "             scoring='f1', verbose=0)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 109
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lfDN-l-WJgPx",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 282
        },
        "outputId": "5d8f2b8b-c519-4cf2-b209-0db76d41c2bd"
      },
      "source": [
        "test=X_train.asdense"
      ],
      "execution_count": 56,
      "outputs": [
        {
          "output_type": "error",
          "ename": "AttributeError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-56-9d6bf9ce51a1>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mtest\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mX_train\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0masdense\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/scipy/sparse/base.py\u001b[0m in \u001b[0;36m__getattr__\u001b[0;34m(self, attr)\u001b[0m\n\u001b[1;32m    686\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgetnnz\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    687\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 688\u001b[0;31m             \u001b[0;32mraise\u001b[0m \u001b[0mAttributeError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mattr\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;34m\" not found\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    689\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    690\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mtranspose\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maxes\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcopy\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mAttributeError\u001b[0m: asdense not found"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eJs7Ngnao4nC",
        "colab_type": "text"
      },
      "source": [
        "###Running the model\n",
        "It's the money time, we can finally run our model!\n",
        "\n",
        "First we need to created it, and train(fit) it."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fGs5g54BNswa",
        "colab_type": "code",
        "outputId": "e4c6a047-acd3-4d99-a21b-f9b89794bff5",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 180
        }
      },
      "source": [
        "clf = REDACTED_MODEL_TYPE(REDACTED_MODEL_PARAMETERS, random_state=42)\n",
        "%time clf.fit(X_train, y_train)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "error",
          "ename": "NameError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-50-e3edcb3a040c>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mclf\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mREDACTED_MODEL_TYPE\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mREDACTED_MODEL_PARAMETERS\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrandom_state\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m42\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0mget_ipython\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmagic\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'time clf.fit(X_train, y_train)'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mNameError\u001b[0m: name 'REDACTED_MODEL_TYPE' is not defined"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1-H91z01wwlI",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 394
        },
        "outputId": "f5178c8b-feb2-401e-f202-7856f3a67e16"
      },
      "source": [
        "# Now we need to get the predictions of our test set\n",
        "%time y_pred = clf.predict(X_test)"
      ],
      "execution_count": 110,
      "outputs": [
        {
          "output_type": "error",
          "ename": "NameError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-110-365c6ea438a8>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mget_ipython\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmagic\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'time y_pred = clf.predict(X_test)'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/IPython/core/interactiveshell.py\u001b[0m in \u001b[0;36mmagic\u001b[0;34m(self, arg_s)\u001b[0m\n\u001b[1;32m   2158\u001b[0m         \u001b[0mmagic_name\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0m_\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmagic_arg_s\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0marg_s\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpartition\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m' '\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2159\u001b[0m         \u001b[0mmagic_name\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmagic_name\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlstrip\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mprefilter\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mESC_MAGIC\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2160\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrun_line_magic\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmagic_name\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmagic_arg_s\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2161\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2162\u001b[0m     \u001b[0;31m#-------------------------------------------------------------------------\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/IPython/core/interactiveshell.py\u001b[0m in \u001b[0;36mrun_line_magic\u001b[0;34m(self, magic_name, line)\u001b[0m\n\u001b[1;32m   2079\u001b[0m                 \u001b[0mkwargs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'local_ns'\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msys\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_getframe\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstack_depth\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mf_locals\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2080\u001b[0m             \u001b[0;32mwith\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbuiltin_trap\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2081\u001b[0;31m                 \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2082\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mresult\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2083\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m</usr/local/lib/python3.6/dist-packages/decorator.py:decorator-gen-60>\u001b[0m in \u001b[0;36mtime\u001b[0;34m(self, line, cell, local_ns)\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/IPython/core/magic.py\u001b[0m in \u001b[0;36m<lambda>\u001b[0;34m(f, *a, **k)\u001b[0m\n\u001b[1;32m    186\u001b[0m     \u001b[0;31m# but it's overkill for just that one bit of state.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    187\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mmagic_deco\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0marg\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 188\u001b[0;31m         \u001b[0mcall\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mlambda\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0ma\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mk\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0ma\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mk\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    189\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    190\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mcallable\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0marg\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/IPython/core/magics/execution.py\u001b[0m in \u001b[0;36mtime\u001b[0;34m(self, line, cell, local_ns)\u001b[0m\n\u001b[1;32m   1191\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1192\u001b[0m             \u001b[0mst\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mclock2\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1193\u001b[0;31m             \u001b[0mexec\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcode\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mglob\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlocal_ns\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1194\u001b[0m             \u001b[0mend\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mclock2\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1195\u001b[0m             \u001b[0mout\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<timed exec>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n",
            "\u001b[0;31mNameError\u001b[0m: name 'clf' is not defined"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-HNTLiSgpn1W",
        "colab_type": "text"
      },
      "source": [
        "###Model evaluation\n",
        "Now that we have our model and it can predict the lead score based on features, we need a way to test if it's any good"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6wglP_6UqOq_",
        "colab_type": "text"
      },
      "source": [
        "####Classification report\n",
        "We use classification_report to get different metrics comparing our prediction to the ground truth."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cpmyj4P-flpW",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "print(classification_report(y_true, y_pred, target_names=['Not Lead', 'Lead']))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aGt_gAIt5iqF",
        "colab_type": "text"
      },
      "source": [
        "We can also get the MCC score of the model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "M_Ax24kc5dgq",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "print('Acc:  {}'.format(metrics.accuracy_score(y_true, y_pred)))\n",
        "print('MCC: {}'.format(metrics.matthews_corrcoef(y_true, y_pred)))\n",
        "print('F1:  {}'.format(metrics.f1_score(y_true, y_pred)))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "B0CC280TpnGV",
        "colab_type": "text"
      },
      "source": [
        "####Plotting the confusion matrix\n",
        "Confusion matrices are useful for comparing our predictions"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hRd1YjZFy-UJ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "fig, axs = plt.subplots(ncols=2, figsize=(14,4))\n",
        "\n",
        "cm = confusion_matrix(y_true, y_pred)\n",
        "ticks = ['Not Lead', 'Lead']\n",
        "cmap = sns.color_palette(\"Blues\")\n",
        "\n",
        "# We normalize our data to see more accurate comparsion\n",
        "sns.heatmap(cm.astype('float') / cm.sum(axis=1)[:, np.newaxis], annot=True, ax=axs[0], cmap=cmap)\n",
        "axs[0].set(title=\"Normalized confusion matrix\", xlabel=\"Prediction\", ylabel=\"Truth\", xticklabels=ticks, yticklabels=ticks)\n",
        "\n",
        "# We also plot the original numbers to get the whole picture\n",
        "sns.heatmap(cm, annot=True, ax=axs[1], fmt='g', cmap=cmap)\n",
        "axs[1].set(title=\"Confusion matrix\", xlabel=\"Prediction\", ylabel=\"Truth\", xticklabels=ticks, yticklabels=ticks);"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0FUIBac_ZRLp",
        "colab_type": "text"
      },
      "source": [
        "###Submitting results\n",
        "After you ran several iterations, and you think your model is good enough, you can send it to us and we'll add your score on the leaderboard!\n",
        "\n",
        "You have to get the results into the following format:\n",
        "```python\n",
        "{\"9023749\": 1, \"9837598\": 0, ...}\n",
        "```\n",
        "\n",
        "This is a dictionary where the keys are `account_id`s and the values are the predicted lead_score.\n",
        "\n",
        "_Make sure you send us **all** the test accounts!_\n",
        "\n",
        "_There should be exactly `71,683` of them!_"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TmB5Vx91ao4t",
        "colab_type": "text"
      },
      "source": [
        "####Prediction\n",
        "First of all, just like before, we have to predict the lead_score.\n",
        "\n",
        "This time you need to use the test set _we_ provided."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8xv469ucaoJ-",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "submission_account_ids = dataset_test.index.values\n",
        "X_submission = preprocess.transform(dataset_test).drop(columns='lead_score')\n",
        "\n",
        "y_pred_submission = clf.predict(X_submission)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CQnIWRVbcltE",
        "colab_type": "text"
      },
      "source": [
        "####Submission\n",
        "Now that we have our submission predictions, we need to pack them up into a compatible format for our server to handle.\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fFfuM8ve8t1w",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Creating a dictionary where the keys are the account_ids\n",
        "# and the values are your predictions\n",
        "prediction = dict(zip(submission_account_ids, y_pred_submission))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zdlT5SRJdadv",
        "colab_type": "text"
      },
      "source": [
        "We now just send the results to our server and wait for the score!"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "E1Ev6lafdZoJ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Importing stuff for http requests\n",
        "from urllib import request\n",
        "import json\n",
        "\n",
        "# We validate first that we actually send all the test accounts expected to be sent\n",
        "if y_pred_submission.shape[0] != 71683 or submission_account_ids.shape[0] != 71683:\n",
        "  raise Exception(\"You have to send all of the accounts! Expected: (71683, 71683), Got: ({}, {})\".format(y_pred_submission.shape[0], submission_account_ids.shape[0]))\n",
        "\n",
        "if \"group_name\" not in vars() or group_name == \"\":\n",
        "  group_name = input(\"Please enter your group's name:\")\n",
        "\n",
        "data = json.dumps({'submitter': group_name, 'predictions': predictions}).encode('utf-8')\n",
        "\n",
        "req = request.Request(f\"https://leaderboard.datahack.org.il/monday/api/\",\n",
        "                      headers={'Content-Type': 'application/json'},\n",
        "                      data=data)\n",
        "\n",
        "res = request.urlopen(req)\n",
        "print(json.load(res))"
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}