{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "DataLearn 2019 - Scaffold.ipynb",
      "version": "0.3.2",
      "provenance": [],
      "collapsed_sections": [],
      "toc_visible": true,
      "machine_shape": "hm",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/debbysonino/LamasDataHack/blob/master/DataLearn_2019_Scaffold_basefilesecondprediction0.45.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dTnHVMK-wyjz",
        "colab_type": "text"
      },
      "source": [
        "&nbsp; ![alt text](https://s3.amazonaws.com/monday.com/static/svg/monday-logos/monday-footer-logo.svg)\n",
        "\n",
        "#Model scaffold\n",
        "This notebook is intended to get you up and running faster.\n",
        "\n",
        "It has the basic scaffold of an ML model, including:\n",
        "* Data loading\n",
        "* Feature extraction\n",
        "* Columns transformation\n",
        "* Training\n",
        "* Evaluating\n",
        "* Submitting results"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jhDZdkDTjyf-",
        "colab_type": "text"
      },
      "source": [
        "###Getting our depnedncies (and data!)\n",
        "First we'll import our relevant libraries"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Gao6_ydqMgBS",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# General DS libraries we are going to need\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "from datetime import timedelta\n",
        "\n",
        "# Importing our base model\n",
        "# [REDACTED ML MODEL USED]\n",
        "\n",
        "# Imports for working with our large dataset\n",
        "from sklearn.utils.random import sample_without_replacement\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "# We need those for data manipulation and getting our features ready for the model\n",
        "from sklearn.preprocessing import OneHotEncoder, Normalizer, Binarizer\n",
        "from sklearn.compose import make_column_transformer\n",
        "\n",
        "# These can be used to measure our model's performance\n",
        "from sklearn import metrics\n",
        "from sklearn.metrics import classification_report, confusion_matrix\n",
        "\n",
        "# Ignore DataFrame assignment warnings\n",
        "pd.options.mode.chained_assignment = None"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "jNoDkmNAsCaY"
      },
      "source": [
        "\n",
        "We set a few constants to use later on for sampeling and running the model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "JhqmBDsksCaa",
        "cellView": "form",
        "colab": {}
      },
      "source": [
        "#@title Model parameters { run: \"auto\" }\n",
        "# n_neighbors = 7 #@param {type:\"slider\", min:1, max:30, step:1}\n",
        "group_name = \"Lamassim\" #@param {type:\"string\"}\n",
        "samples_num = 230000 #@param {type:\"slider\", min:0, max:1500000, step:10000}\n",
        "n_jobs = -1 #@param {type:\"slider\", min:-1, max:32, step:1}\n",
        "path_prefix = \"https://storage.googleapis.com/mondaycom-datahack/final_sets\" #@param [\"https://storage.googleapis.com/mondaycom-datahack/final_sets\", \"https://mondaycom-datahack.s3.amazonaws.com/final_sets\"] {allow-input: true}"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "DpkqHB9lsUmQ"
      },
      "source": [
        "Next we'll load all the different parts of our dataset\n",
        "\n",
        "<br/>\n",
        "\n",
        "_Our use my data loading [snippet](https://colab.research.google.com/drive/1_Y-sZ5eHIDlDUMuLCwfnbuJdLh0DTXmO#scrollTo=5HGlaJTEAYJu&line=23&uniqifier=1)!_"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "b2x0lOgBZRP_",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 230
        },
        "outputId": "9b8550ac-c434-4086-da47-4de2d87ff70e"
      },
      "source": [
        "import os\n",
        "import pandas as pd\n",
        "\n",
        "# We define the datasets we want to load\n",
        "datasets = ('accounts', 'users', 'events', 'subscriptions')\n",
        "source_prefix = 'https://storage.googleapis.com/mondaycom-datahack/final_sets/'\n",
        "\n",
        "local_dir = './datasets/datahack/'\n",
        "file_prefix = 'train_'\n",
        "file_suffix = ''\n",
        "file_extension = 'csv'\n",
        "\n",
        "# We create a directory for the datasets if it doesn't exist\n",
        "if not os.path.exists(local_dir):\n",
        "    os.makedirs(local_dir)\n",
        "\n",
        "# For each dataset we want, we check if we already downloaded it and fix it if we didn't\n",
        "for dataset in datasets:\n",
        "  if not os.path.isfile('{}{}{}{}.{}'.format(local_dir, file_prefix, dataset, file_suffix, file_extension)):\n",
        "    !curl {source_prefix}{file_prefix}{dataset}{file_suffix}.{file_extension} --output {local_dir}{file_prefix}{dataset}{file_suffix}.{file_extension}\n",
        "\n",
        "  # Load the datasets into a DataFrame using pandas\n",
        "  globals()['{}{}'.format(file_prefix, dataset)] = pd.read_csv('{}{}{}{}.{}'.format(local_dir, file_prefix, dataset, file_suffix, file_extension), low_memory=False)"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "  % Total    % Received % Xferd  Average Speed   Time    Time     Time  Current\n",
            "                                 Dload  Upload   Total   Spent    Left  Speed\n",
            "100  204M  100  204M    0     0  32.1M      0  0:00:06  0:00:06 --:--:-- 53.1M\n",
            "  % Total    % Received % Xferd  Average Speed   Time    Time     Time  Current\n",
            "                                 Dload  Upload   Total   Spent    Left  Speed\n",
            "100  330M  100  330M    0     0   279M      0  0:00:01  0:00:01 --:--:--  279M\n",
            "  % Total    % Received % Xferd  Average Speed   Time    Time     Time  Current\n",
            "                                 Dload  Upload   Total   Spent    Left  Speed\n",
            "100 1215M  100 1215M    0     0   170M      0  0:00:07  0:00:07 --:--:--  183M\n",
            "  % Total    % Received % Xferd  Average Speed   Time    Time     Time  Current\n",
            "                                 Dload  Upload   Total   Spent    Left  Speed\n",
            "100 17.1M  100 17.1M    0     0   132M      0 --:--:-- --:--:-- --:--:--  132M\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KdEFqnpfKjmk",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 230
        },
        "outputId": "2e85283d-2f47-42b0-b4be-52f7b8e14e1d"
      },
      "source": [
        "import os\n",
        "import pandas as pd\n",
        "\n",
        "# We define the datasets we want to load\n",
        "datasets = ('accounts', 'users', 'events', 'subscriptions')\n",
        "source_prefix = 'https://storage.googleapis.com/mondaycom-datahack/final_sets/'\n",
        "\n",
        "local_dir = './datasets/datahack/'\n",
        "file_prefix = 'test_'\n",
        "file_suffix = ''\n",
        "file_extension = 'csv'\n",
        "\n",
        "# We create a directory for the datasets if it doesn't exist\n",
        "if not os.path.exists(local_dir):\n",
        "    os.makedirs(local_dir)\n",
        "\n",
        "# For each dataset we want, we check if we already downloaded it and fix it if we didn't\n",
        "for dataset in datasets:\n",
        "  if not os.path.isfile('{}{}{}{}.{}'.format(local_dir, file_prefix, dataset, file_suffix, file_extension)):\n",
        "    !curl {source_prefix}{file_prefix}{dataset}{file_suffix}.{file_extension} --output {local_dir}{file_prefix}{dataset}{file_suffix}.{file_extension}\n",
        "\n",
        "  # Load the datasets into a DataFrame using pandas\n",
        "  globals()['{}{}'.format(file_prefix, dataset)] = pd.read_csv('{}{}{}{}.{}'.format(local_dir, file_prefix, dataset, file_suffix, file_extension), low_memory=False)"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "  % Total    % Received % Xferd  Average Speed   Time    Time     Time  Current\n",
            "                                 Dload  Upload   Total   Spent    Left  Speed\n",
            "100 10.6M  100 10.6M    0     0  68.6M      0 --:--:-- --:--:-- --:--:-- 68.6M\n",
            "  % Total    % Received % Xferd  Average Speed   Time    Time     Time  Current\n",
            "                                 Dload  Upload   Total   Spent    Left  Speed\n",
            "100 17.1M  100 17.1M    0     0   100M      0 --:--:-- --:--:-- --:--:--  100M\n",
            "  % Total    % Received % Xferd  Average Speed   Time    Time     Time  Current\n",
            "                                 Dload  Upload   Total   Spent    Left  Speed\n",
            "100 63.2M  100 63.2M    0     0   118M      0 --:--:-- --:--:-- --:--:--  118M\n",
            "  % Total    % Received % Xferd  Average Speed   Time    Time     Time  Current\n",
            "                                 Dload  Upload   Total   Spent    Left  Speed\n",
            "100  959k  100  959k    0     0  27.5M      0 --:--:-- --:--:-- --:--:-- 27.5M\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yh1wGtOsUcnf",
        "colab_type": "text"
      },
      "source": [
        "We need to add our test sets to our train sets and work on both at the same time.\n",
        "\n",
        "We'll split them back up before training and inference."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JNcD_YSBUbiY",
        "colab_type": "code",
        "outputId": "8c44f957-3bb2-4f65-c244-242dc3bda58b",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 178
        }
      },
      "source": [
        "accounts = train_accounts.append(test_accounts)\n",
        "users = train_users.append(test_users)\n",
        "events = train_events.append(test_events)\n",
        "subscriptions = train_subscriptions.append(test_subscriptions)"
      ],
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/pandas/core/frame.py:6692: FutureWarning: Sorting because non-concatenation axis is not aligned. A future version\n",
            "of pandas will change to not sort by default.\n",
            "\n",
            "To accept the future behavior, pass 'sort=False'.\n",
            "\n",
            "To retain the current behavior and silence the warning, pass 'sort=True'.\n",
            "\n",
            "  sort=sort)\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Jo5vLhgAlY-K",
        "colab_type": "text"
      },
      "source": [
        "###Feature engineering\n",
        "In this block we add a new feature of `[REDACTED]` extracted from the user `[REDACTED]`\n",
        "\n",
        "We also seperate all the `[REDACTED]` users into a different DataFrame"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "w6VUhsrzlvES",
        "colab_type": "text"
      },
      "source": [
        "Let's enrich our data a bit"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0f9X2owrCpl0",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "accounts[\"numuser\"]=users.groupby(\"account_id\").user_id.size()\n",
        "accounts[\"numisadmin\"]=users.groupby(\"account_id\").is_admin.sum()\n",
        "accounts[\"numenabled\"]=users.groupby(\"account_id\").enabled.sum()\n",
        "accounts[\"numpending\"]=users.groupby(\"account_id\").pending.sum()\n",
        "accounts[\"numcountry\"]=users.groupby(\"account_id\").country.count()\n",
        "accounts[\"numregion\"]=users.groupby(\"account_id\").region.count()\n",
        "accounts[\"numrcity\"]=users.groupby(\"account_id\").city.count()\n",
        "accounts[\"numphoto\"]=users.groupby(\"account_id\").has_photo.sum()\n",
        "accounts[\"nummos\"]=users.groupby(\"account_id\").os.count()\n",
        "accounts[\"nummobile\"]=users.groupby(\"account_id\").device.count()\n",
        "accounts[\"nummobile2\"]=users[users[\"device\"]==\"mobile\"].groupby(\"account_id\").device.count()\n",
        "accounts[\"numchrome\"]=users[users[\"browser\"]==\"chrome\"].groupby(\"account_id\").browser.count()\n",
        "#accounts[\"vetekuser\"]=users.groupby(\"account_id\").created_at.min()\n",
        "#accounts[\"vetekactive\"]=users.groupby(\"account_id\").became_active_at.min()\n",
        "\n",
        "\n",
        "#accounts[\"vetekuser\"]=users.groupby(\"account_id\").created_at.min()\n",
        "#accounts[\"vetekactive\"]=users.groupby(\"account_id\").became_active_at.min()\n",
        "\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nHHiQGXtbXxg",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#sub_df = pd.read_csv('https://storage.googleapis.com/mondaycom-datahack/final_sets/train_subscriptions.csv', nrows=7000000000)\n",
        "def upgrade(row):\n",
        "  return abs(row[\"plan_id\"]-row[\"prev_plan_id\"])>0\n",
        "\n",
        "\n",
        "subscriptions[\"plan_upgrade\"] =  subscriptions.apply(upgrade ,axis=1)\n",
        "\n",
        "subscriptions['status_success']=subscriptions['status'].fillna('SUCCESS')\n",
        "subscriptions['status_success']=subscriptions['status_success']=='SUCCESS'\n",
        "#sub_df[sub_df['status_success']==0].sum() # check if mrr=0 for status=failed\n",
        "\n",
        "account_subs=subscriptions.groupby('account_id').agg({\"subscription_id\":\"count\",\"plan_id\":\"nunique\", \"plan_upgrade\": \"mean\",\"mrr_gain\":\"sum\"})\n",
        "account_subs.rename(columns={'subscription_id':'sub_count',\n",
        "                          'plan_id':'unique_plan',\n",
        "                          \n",
        "                             'mrr_gain':'mrr_gain_sum'}, \n",
        "                 inplace=True)\n",
        "accounts=pd.merge(accounts, account_subs, on='account_id', how='left')\n",
        "#events_by_account_user = events.groupby(['account_id'])['payment_events'].sum()\n",
        "accounts['browser'] = accounts['browser'].fillna('chrome')\n",
        "accounts['churn_reason'] = accounts['churn_reason'].fillna(0)\n",
        "accounts['payment_events_by_account']=events.groupby(['account_id'])['payment_events'].sum()\n",
        "accounts['payment_events_by_account'] = accounts['payment_events_by_account'].fillna(0)\n",
        "accounts['last_event_day']=events.groupby(['account_id'])['date'].max()\n",
        "# accounts['last_event_day'].fillna(accounts['subscription_started_at'])\n",
        "# from the resume data\n",
        "accounts['Avg_event_by_total']=(events.groupby(['account_id']).total_events.sum()/events.groupby(['account_id']).date.count())\n",
        "accounts['Avg_event_by_column']=(events.groupby(['account_id']).column_events.sum()/events.groupby(['account_id']).date.count())\n",
        "accounts['Avg_event_by_board']=(events.groupby(['account_id']).board_events.sum()/events.groupby(['account_id']).date.count())\n",
        "accounts['Avg_event_by_num']=(events.groupby(['account_id']).num_of_boards.sum()/events.groupby(['account_id']).date.count())\n",
        "accounts['Avg_event_by_count']=(events.groupby(['account_id']).count_kind_columns.sum()/events.groupby(['account_id']).date.count())\n",
        "accounts['Avg_event_by_content']=(events.groupby(['account_id']).content_events.sum()/events.groupby(['account_id']).date.count())\n",
        "accounts['Avg_event_by_group']=(events.groupby(['account_id']).group_events.sum()/events.groupby(['account_id']).date.count())\n",
        "accounts['Avg_event_by_invite']=(events.groupby(['account_id']).invite_events.sum()/events.groupby(['account_id']).date.count())\n",
        "accounts['Avg_event_by_import']=(events.groupby(['account_id']).import_events.sum()/events.groupby(['account_id']).date.count())\n",
        "accounts['Avg_event_by_notification']=(events.groupby(['account_id']).notification_events.sum()/events.groupby(['account_id']).date.count())\n",
        "accounts['Avg_event_by_new']=(events.groupby(['account_id']).new_entry_events.sum()/events.groupby(['account_id']).date.count())\n",
        "accounts['Avg_event_by_payment']=(events.groupby(['account_id']).payment_events.sum()/events.groupby(['account_id']).date.count())\n",
        "accounts['Avg_event_by_inbox']=(events.groupby(['account_id']).inbox_events.sum()/events.groupby(['account_id']).date.count())\n",
        "accounts['Avg_event_by_communicating']=(events.groupby(['account_id']).communicating_events.sum()/events.groupby(['account_id']).date.count())\n",
        "accounts['Avg_event_by_non']=(events.groupby(['account_id']).non_communicating_events.sum()/events.groupby(['account_id']).date.count())\n",
        "accounts['Avg_event_by_web']=(events.groupby(['account_id']).web_events.sum()/events.groupby(['account_id']).date.count())\n",
        "accounts['Avg_event_by_ios']=(events.groupby(['account_id']).ios_events.sum()/events.groupby(['account_id']).date.count())\n",
        "accounts['Avg_event_by_android']=(events.groupby(['account_id']).android_events.sum()/events.groupby(['account_id']).date.count())\n",
        "accounts['Avg_event_by_desktop']=(events.groupby(['account_id']).desktop_app_events.sum()/events.groupby(['account_id']).date.count())\n",
        "accounts['Avg_event_by_empty']=(events.groupby(['account_id']).empty_events.sum()/events.groupby(['account_id']).date.count())\n",
        "\n",
        "\n",
        "accounts['P_event_by_column']=events.groupby(['account_id']).column_events.sum()/(events.groupby(['account_id']).total_events.sum())                                                                   \n",
        "accounts['P_event_by_board']=events.groupby(['account_id'])['board_events'].sum()/(events.groupby(['account_id']).total_events.sum())                                                 \n",
        "accounts['P_event_by_num']=events.groupby(['account_id']).num_of_boards.sum()/(events.groupby(['account_id']).total_events.sum())\n",
        "accounts['P_event_by_count']=events.groupby(['account_id']).count_kind_columns.sum()/(events.groupby(['account_id']).total_events.sum())\n",
        "accounts['P_event_by_content']=events.groupby(['account_id']).content_events.sum()/(events.groupby(['account_id']).total_events.sum())\n",
        "accounts['P_event_by_group']=events.groupby(['account_id']).group_events.sum()/(events.groupby(['account_id']).total_events.sum())\n",
        "accounts['P_event_by_invite']=events.groupby(['account_id']).invite_events.sum()/(events.groupby(['account_id']).total_events.sum())\n",
        "accounts['P_event_by_import']=events.groupby(['account_id']).import_events.sum()/(events.groupby(['account_id']).total_events.sum())                                                                \n",
        "accounts['P_event_by_notification']=events.groupby(['account_id']).notification_events.sum()/(events.groupby(['account_id']).total_events.sum())\n",
        "accounts['P_event_by_new']=events.groupby(['account_id']).new_entry_events.sum()/(events.groupby(['account_id']).total_events.sum())\n",
        "accounts['P_event_by_payment']=events.groupby(['account_id']).payment_events.sum()/(events.groupby(['account_id']).total_events.sum())\n",
        "accounts['P_event_by_inbox']=events.groupby(['account_id']).inbox_events.sum()/(events.groupby(['account_id']).total_events.sum())\n",
        "accounts['P_event_by_communicating']=events.groupby(['account_id']).communicating_events.sum()/(events.groupby(['account_id']).total_events.sum())\n",
        "accounts['P_event_by_non']=events.groupby(['account_id']).non_communicating_events.sum()/(events.groupby(['account_id']).total_events.sum())\n",
        "accounts['P_event_by_web']=events.groupby(['account_id']).web_events.sum()/(events.groupby(['account_id']).total_events.sum())\n",
        "accounts['P_event_by_ios']=events.groupby(['account_id']).ios_events.sum()/(events.groupby(['account_id']).total_events.sum())\n",
        "accounts['P_event_by_android']=events.groupby(['account_id']).android_events.sum()/(events.groupby(['account_id']).total_events.sum())\n",
        "accounts['P_event_by_desktop']=events.groupby(['account_id']).desktop_app_events.sum()/(events.groupby(['account_id']).total_events.sum())\n",
        "accounts['P_event_by_empty']=events.groupby(['account_id']).empty_events.sum()/(events.groupby(['account_id']).total_events.sum())\n",
        "\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "H1cSkHTJd94Q",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "popular_emails = pd.read_csv('https://raw.githubusercontent.com/debbysonino/LamasDataHack/master/popular_emails.csv', nrows=7000000000)\n",
        "users['email'].value_counts()\n",
        "users['domain']=users['email'].apply(lambda x: x.split('@')[-1])\n",
        "users['mailsuffix']=users['domain'].apply(lambda x: x.split('.')[1])\n",
        "users['domain_indic']=users['domain'].isin(popular_emails)\n",
        "accounts['domain']=1-users.groupby('account_id').domain_indic.mean()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4C83j6hKrjZL",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "accounts['time_between_created_trail'] = (pd.to_datetime(accounts['trial_start']) - pd.to_datetime(accounts['created_at'])).apply(lambda x: (x.seconds//3600))\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LGJOxNPMuJbw",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import time\n",
        "import datetime\n",
        "accounts['day_start']=accounts['created_at'].apply(lambda x: pd.to_datetime(x).strftime(\"%A\"))\n",
        "accounts['is_weekend']=accounts['day_start'].isin(['Saturday','Sunday'])"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lXz0mQBiu41V",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "accounts['sum_event_by_account']=(events.groupby(['account_id']).total_events.sum())"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VDsDxbRk6km5",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 327
        },
        "outputId": "0a81f745-b953-4733-b02b-80607316e5ef"
      },
      "source": [
        "\n"
      ],
      "execution_count": 316,
      "outputs": [
        {
          "output_type": "error",
          "ename": "AttributeError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-316-d18097070b85>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0msubscriptions\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'EXTERNAL'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0msubscriptions\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'payment_type'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m==\u001b[0m\u001b[0;34m'EXTERNAL'\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m \u001b[0msubscriptions\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mSUM\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'EXTERNAL'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/pandas/core/generic.py\u001b[0m in \u001b[0;36m__getattr__\u001b[0;34m(self, name)\u001b[0m\n\u001b[1;32m   5065\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_info_axis\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_can_hold_identifiers_and_holds_name\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   5066\u001b[0m                 \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 5067\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mobject\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__getattribute__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   5068\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   5069\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__setattr__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mAttributeError\u001b[0m: 'DataFrame' object has no attribute 'SUM'"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-u2yzKUsKqsa",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "all_features=accounts"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Sc13u-BH6ogn",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 123
        },
        "outputId": "af92ee4c-367e-4167-e316-f581df4c6926"
      },
      "source": [
        "subscriptions.columns"
      ],
      "execution_count": 309,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "Index(['event_happened_at', 'subscription_id', 'account_id', 'plan_id',\n",
              "       'event_type', 'invoice_charge_amount', 'prev_plan_id', 'status',\n",
              "       'status_reason', 'currency', 'invoice_charge_amount_usd', 'mrr_gain',\n",
              "       'next_charge_date', 'payment_type', 'transaction_date', 'plan_upgrade',\n",
              "       'status_success'],\n",
              "      dtype='object')"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 309
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SngUmaPImHZF",
        "colab_type": "text"
      },
      "source": [
        "###Data preperation\n",
        "After we created our raw features we need to make sure the fit the way our ML model expects to receive them."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qMEWXPofVXQp",
        "colab_type": "text"
      },
      "source": [
        "###Re-splitting\n",
        "We now need to split our data back to the original train set and test set.\n",
        "\n",
        "We also make sure we keep only the columns we want in the data frame (the features)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DeYB9q5Hmdtl",
        "colab_type": "text"
      },
      "source": [
        "###Setting everything up\n",
        "Our dataset is large (1,500,000+ accounts, each has a few users, each has events for every day)\n",
        "\n",
        "We need to work on a smaller batch of the training data so we can iterate more quickly.\n",
        "\n",
        "Once we find a good architecture we can increase the sample size to increase the accuracy."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "35doXb6oE99b",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# We map our features into different types\n",
        "categorical_features = ['country' , 'device','industry','os','user_goal', 'churn_reason' ]\n",
        "# status_success\n",
        "\n",
        "normalized_features = [ 'company_size', 'max_team_size','mrr',\n",
        "                       \"numuser\",   \"numenabled\", \n",
        "                    \"numrcity\",  \"numphoto\", \"nummos\", \"nummobile\", 'time_diff',  \"mrr_gain_sum\", \n",
        "                       'payment_events_by_account', 'domain'\n",
        "                      ]\n",
        "\n",
        "\n",
        "\n",
        "binary_features = ['paying', 'has_logo', \"plan_upgrade\"]\n",
        "\n",
        "untouched_features = ['account_id']\n",
        "\n",
        "target = ['lead_score']\n",
        "\n",
        "# And create a column transformer to handle the manipulation for us\n",
        "preprocess = make_column_transformer(\n",
        "    (OneHotEncoder(), categorical_features+['plan_id']),\n",
        "    (Normalizer(), normalized_features),\n",
        "    (Binarizer(), binary_features)\n",
        ")\n",
        "# Getting only the relevant features from the dataset\n",
        "dataset = all_features[categorical_features + normalized_features + binary_features + ['plan_id'] + untouched_features + target]\n",
        "\n",
        "# Filling empty values with default values \n",
        "dataset.loc[:,categorical_features] = dataset[categorical_features].fillna('')\n",
        "dataset.loc[:,'plan_id'] = dataset['plan_id'].fillna(0)\n",
        "\n",
        "dataset.loc[:,normalized_features +\n",
        "              binary_features +\n",
        "              untouched_features] = dataset[normalized_features +\n",
        "                                            binary_features +\n",
        "                                            untouched_features].fillna(0)\n",
        "\n",
        "# Splitting them back up to the original train/test split\n",
        "dataset_train = dataset[dataset.reset_index().account_id.isin(train_accounts.account_id)]\n",
        "dataset_test = dataset[dataset.reset_index().account_id.isin(test_accounts.account_id)]\n",
        "# Splitting them back up to the original train/test split\n",
        "dataset_train = dataset.loc[~(dataset['lead_score'].isna())].reset_index().drop(['index'],axis=1)\n",
        "dataset_test = dataset.loc[(dataset['lead_score'].isna())].reset_index().drop(['index'],axis=1)\n",
        "sampled_dataset_train = dataset_train.iloc[sample_without_replacement(dataset_train.shape[0], samples_num)]\n",
        "# We fit our column transformer on both the train and the test sets\n",
        "preprocess.fit(sampled_dataset_train.append(dataset_test))\n",
        "\n",
        "# We use transform to finally manipulate the features of our training set\n",
        "X = preprocess.transform(sampled_dataset_train)\n",
        "\n",
        "# Seperating the label\n",
        "y = sampled_dataset_train.pop('lead_score')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ilVMJZclfOr8",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "sampled_dataset_train = dataset_train.iloc[sample_without_replacement(dataset_train.shape[0], samples_num)]\n",
        "# We fit our column transformer on both the train and the test sets\n",
        "preprocess.fit(sampled_dataset_train.append(dataset_test))\n",
        "\n",
        "# We use transform to finally manipulate the features of our training set\n",
        "X = preprocess.transform(sampled_dataset_train)\n",
        "\n",
        "# Seperating the label\n",
        "y = sampled_dataset_train.pop('lead_score')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hglqe5Tymm_o",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "885618f0-828b-4ef4-f25d-adbd700d4438"
      },
      "source": [
        "y.mean()"
      ],
      "execution_count": 337,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0.024665217391304346"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 337
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2oOxDhLgM8a4",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# You now need to split the data into YOUR OWN training and test sets\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.05, random_state=42)\n",
        "\n",
        "# For standardization purposes we store y_test in a y_true variable\n",
        "y_true = y_test"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "h49yMjV2HMow",
        "colab_type": "code",
        "outputId": "27526b61-4e9a-44e9-d875-75a0e0eac81b",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 481
        }
      },
      "source": [
        "## (when using google colab)\n",
        "! pip install catboost\n",
        "! pip install plotly_express"
      ],
      "execution_count": 339,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: catboost in /usr/local/lib/python3.6/dist-packages (0.16.5)\n",
            "Requirement already satisfied: pandas>=0.19.1 in /usr/local/lib/python3.6/dist-packages (from catboost) (0.24.2)\n",
            "Requirement already satisfied: graphviz in /usr/local/lib/python3.6/dist-packages (from catboost) (0.10.1)\n",
            "Requirement already satisfied: matplotlib in /usr/local/lib/python3.6/dist-packages (from catboost) (3.0.3)\n",
            "Requirement already satisfied: plotly in /usr/local/lib/python3.6/dist-packages (from catboost) (4.1.1)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.6/dist-packages (from catboost) (1.12.0)\n",
            "Requirement already satisfied: numpy>=1.16.0 in /usr/local/lib/python3.6/dist-packages (from catboost) (1.16.4)\n",
            "Requirement already satisfied: pytz>=2011k in /usr/local/lib/python3.6/dist-packages (from pandas>=0.19.1->catboost) (2018.9)\n",
            "Requirement already satisfied: python-dateutil>=2.5.0 in /usr/local/lib/python3.6/dist-packages (from pandas>=0.19.1->catboost) (2.5.3)\n",
            "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.6/dist-packages (from matplotlib->catboost) (0.10.0)\n",
            "Requirement already satisfied: kiwisolver>=1.0.1 in /usr/local/lib/python3.6/dist-packages (from matplotlib->catboost) (1.1.0)\n",
            "Requirement already satisfied: pyparsing!=2.0.4,!=2.1.2,!=2.1.6,>=2.0.1 in /usr/local/lib/python3.6/dist-packages (from matplotlib->catboost) (2.4.2)\n",
            "Requirement already satisfied: retrying>=1.3.3 in /usr/local/lib/python3.6/dist-packages (from plotly->catboost) (1.3.3)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.6/dist-packages (from kiwisolver>=1.0.1->matplotlib->catboost) (41.2.0)\n",
            "Requirement already satisfied: plotly_express in /usr/local/lib/python3.6/dist-packages (0.4.1)\n",
            "Requirement already satisfied: plotly>=4.1.0 in /usr/local/lib/python3.6/dist-packages (from plotly_express) (4.1.1)\n",
            "Requirement already satisfied: statsmodels>=0.9.0 in /usr/local/lib/python3.6/dist-packages (from plotly_express) (0.10.1)\n",
            "Requirement already satisfied: pandas>=0.20.0 in /usr/local/lib/python3.6/dist-packages (from plotly_express) (0.24.2)\n",
            "Requirement already satisfied: numpy>=1.11 in /usr/local/lib/python3.6/dist-packages (from plotly_express) (1.16.4)\n",
            "Requirement already satisfied: patsy>=0.5 in /usr/local/lib/python3.6/dist-packages (from plotly_express) (0.5.1)\n",
            "Requirement already satisfied: scipy>=0.18 in /usr/local/lib/python3.6/dist-packages (from plotly_express) (1.3.1)\n",
            "Requirement already satisfied: retrying>=1.3.3 in /usr/local/lib/python3.6/dist-packages (from plotly>=4.1.0->plotly_express) (1.3.3)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.6/dist-packages (from plotly>=4.1.0->plotly_express) (1.12.0)\n",
            "Requirement already satisfied: python-dateutil>=2.5.0 in /usr/local/lib/python3.6/dist-packages (from pandas>=0.20.0->plotly_express) (2.5.3)\n",
            "Requirement already satisfied: pytz>=2011k in /usr/local/lib/python3.6/dist-packages (from pandas>=0.20.0->plotly_express) (2018.9)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "khj17scYHQnH",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "## General\n",
        "import os \n",
        "import joblib\n",
        "import requests\n",
        "from google_drive_downloader import GoogleDriveDownloader as gdd\n",
        "\n",
        "## Data manipulation\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "\n",
        "## Modeling\n",
        "### Modeling pipeline\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.model_selection import cross_val_score\n",
        "from sklearn.metrics import classification_report, confusion_matrix, accuracy_score, precision_score, recall_score, f1_score\n",
        "from sklearn.model_selection import GridSearchCV\n",
        " \n",
        "### Models\n",
        "from sklearn.tree import DecisionTreeClassifier\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "from xgboost import XGBClassifier    \n",
        "from catboost import CatBoostClassifier, Pool\n",
        "\n",
        "## Visuatlization\n",
        "import plotly \n",
        "import plotly.express as px\n",
        "import plotly.graph_objects as go"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lU_fiOvFHE0p",
        "colab_type": "code",
        "outputId": "c4e0b6a1-4670-4b67-fdc3-dbc507bbeb73",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 372
        }
      },
      "source": [
        "parameters = {'n_estimators': [50], #[50,100,200]\n",
        "             'max_depth': [40], #[5,10,15],\n",
        "             'criterion': ['gini', 'entropy'],\n",
        "             'max_features': [20]} #[int(np.log2(X_train.shape[1])), int(np.sqrt(X_train.shape[1]))]\n",
        "\n",
        "rf_cv = GridSearchCV(RandomForestClassifier(class_weight='balanced'), \n",
        "                   parameters, \n",
        "                   n_jobs = -1,\n",
        "                   cv = 5,\n",
        "                   refit = True,\n",
        "                   scoring = 'f1')\n",
        "\n",
        "rf_cv.fit(X_train, y_train)"
      ],
      "execution_count": 341,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "GridSearchCV(cv=5, error_score='raise-deprecating',\n",
              "             estimator=RandomForestClassifier(bootstrap=True,\n",
              "                                              class_weight='balanced',\n",
              "                                              criterion='gini', max_depth=None,\n",
              "                                              max_features='auto',\n",
              "                                              max_leaf_nodes=None,\n",
              "                                              min_impurity_decrease=0.0,\n",
              "                                              min_impurity_split=None,\n",
              "                                              min_samples_leaf=1,\n",
              "                                              min_samples_split=2,\n",
              "                                              min_weight_fraction_leaf=0.0,\n",
              "                                              n_estimators='warn', n_jobs=None,\n",
              "                                              oob_score=False,\n",
              "                                              random_state=None, verbose=0,\n",
              "                                              warm_start=False),\n",
              "             iid='warn', n_jobs=-1,\n",
              "             param_grid={'criterion': ['gini', 'entropy'], 'max_depth': [40],\n",
              "                         'max_features': [20], 'n_estimators': [50]},\n",
              "             pre_dispatch='2*n_jobs', refit=True, return_train_score=False,\n",
              "             scoring='f1', verbose=0)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 341
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MQDz6eEvcGLJ",
        "colab_type": "code",
        "outputId": "f50fe598-a18c-4242-87df-8f87a06c6f9d",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 141
        }
      },
      "source": [
        "rf_best_model = rf_cv.best_estimator_\n",
        "rf_best_model"
      ],
      "execution_count": 342,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "RandomForestClassifier(bootstrap=True, class_weight='balanced',\n",
              "                       criterion='gini', max_depth=40, max_features=20,\n",
              "                       max_leaf_nodes=None, min_impurity_decrease=0.0,\n",
              "                       min_impurity_split=None, min_samples_leaf=1,\n",
              "                       min_samples_split=2, min_weight_fraction_leaf=0.0,\n",
              "                       n_estimators=50, n_jobs=None, oob_score=False,\n",
              "                       random_state=None, verbose=0, warm_start=False)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 342
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4QW73bMecL9F",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "y_train_pred = rf_best_model.predict(X_train) "
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nReMIThzcPnk",
        "colab_type": "code",
        "outputId": "d1168534-c3a2-4c86-f56e-9fb27585aa62",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 87
        }
      },
      "source": [
        "accuracy_score(y_train_pred, y_train),f1_score(y_train, y_train_pred),precision_score(y_train, y_train_pred),recall_score(y_train, y_train_pred)"
      ],
      "execution_count": 344,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(0.9762105263157895,\n",
              " 0.6732461654513452,\n",
              " 0.5089820359281437,\n",
              " 0.9940597735288658)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 344
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kFMf9ez3cgo8",
        "colab_type": "code",
        "outputId": "ee4b2b1f-ab3b-49b6-e8e2-b1c605643865",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 52
        }
      },
      "source": [
        "confusion_matrix(y_train_pred, y_train)\n",
        "\n"
      ],
      "execution_count": 345,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([[207947,     32],\n",
              "       [  5166,   5355]])"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 345
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GeEMwslgdJGF",
        "colab_type": "text"
      },
      "source": [
        "## XGBOOST"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eJs7Ngnao4nC",
        "colab_type": "text"
      },
      "source": [
        "###Running the model\n",
        "It's the money time, we can finally run our model!\n",
        "\n",
        "First we need to created it, and train(fit) it."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1-H91z01wwlI",
        "colab_type": "code",
        "outputId": "79e25bd5-bbb4-4d99-a5f1-944a39a6e04b",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 52
        }
      },
      "source": [
        "# Now we need to get the predictions of our test set\n",
        "%time y_pred = rf_best_model.predict(X_test)"
      ],
      "execution_count": 346,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "CPU times: user 103 ms, sys: 964 Âµs, total: 104 ms\n",
            "Wall time: 103 ms\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-HNTLiSgpn1W",
        "colab_type": "text"
      },
      "source": [
        "###Model evaluation\n",
        "Now that we have our model and it can predict the lead score based on features, we need a way to test if it's any good"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6wglP_6UqOq_",
        "colab_type": "text"
      },
      "source": [
        "####Classification report\n",
        "We use classification_report to get different metrics comparing our prediction to the ground truth."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cpmyj4P-flpW",
        "colab_type": "code",
        "outputId": "8f677eb8-6849-4fa1-b9fa-ca9382a53e59",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 176
        }
      },
      "source": [
        "print(classification_report(y_true, y_pred, target_names=['Not Lead', 'Lead']))"
      ],
      "execution_count": 347,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "              precision    recall  f1-score   support\n",
            "\n",
            "    Not Lead       0.99      0.97      0.98     11214\n",
            "        Lead       0.33      0.62      0.43       286\n",
            "\n",
            "    accuracy                           0.96     11500\n",
            "   macro avg       0.66      0.79      0.70     11500\n",
            "weighted avg       0.97      0.96      0.96     11500\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "W3B2N7eblo0v",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "7314ad73-6e66-4645-cd9f-2813f51b804f"
      },
      "source": [
        "importances = rf_best_model.feature_importances_\n",
        "std = np.std([tree.feature_importances_ for tree in rf_best_model.estimators_],\n",
        "             axis=0)\n",
        "indices = np.argsort(importances)[::-1]\n",
        "\n",
        "# Print the feature ranking\n",
        "print(\"Feature ranking:\")"
      ],
      "execution_count": 348,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Feature ranking:\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aGt_gAIt5iqF",
        "colab_type": "text"
      },
      "source": [
        "We can also get the MCC score of the model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "M_Ax24kc5dgq",
        "colab_type": "code",
        "outputId": "bcc9aadc-9297-4675-ca4f-5c4874f6d788",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 70
        }
      },
      "source": [
        "print('Acc:  {}'.format(metrics.accuracy_score(y_true, y_pred)))\n",
        "print('MCC: {}'.format(metrics.matthews_corrcoef(y_true, y_pred)))\n",
        "print('F1:  {}'.format(metrics.f1_score(y_true, y_pred)))"
      ],
      "execution_count": 349,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Acc:  0.9586956521739131\n",
            "MCC: 0.4321690382075961\n",
            "F1:  0.42839951865222625\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "B0CC280TpnGV",
        "colab_type": "text"
      },
      "source": [
        "####Plotting the confusion matrix\n",
        "Confusion matrices are useful for comparing our predictions"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0FUIBac_ZRLp",
        "colab_type": "text"
      },
      "source": [
        "###Submitting results\n",
        "After you ran several iterations, and you think your model is good enough, you can send it to us and we'll add your score on the leaderboard!\n",
        "\n",
        "You have to get the results into the following format:\n",
        "```python\n",
        "{\"9023749\": 1, \"9837598\": 0, ...}\n",
        "```\n",
        "\n",
        "This is a dictionary where the keys are `account_id`s and the values are the predicted lead_score.\n",
        "\n",
        "_Make sure you send us **all** the test accounts!_\n",
        "\n",
        "_There should be exactly `71,683` of them!_"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TmB5Vx91ao4t",
        "colab_type": "text"
      },
      "source": [
        "####Prediction\n",
        "First of all, just like before, we have to predict the lead_score.\n",
        "\n",
        "This time you need to use the test set _we_ provided."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8xv469ucaoJ-",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "dataset_test = dataset[dataset['lead_score'].isna()]\n",
        "submission_account_ids = dataset_test.account_id\n",
        "\n",
        "X_submission = preprocess.transform(dataset_test)\n",
        "\n",
        "y_pred_submission = rf_best_model.predict(X_submission)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CQnIWRVbcltE",
        "colab_type": "text"
      },
      "source": [
        "####Submission\n",
        "Now that we have our submission predictions, we need to pack them up into a compatible format for our server to handle.\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fFfuM8ve8t1w",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Creating a dictionary where the keys are the account_ids\n",
        "# and the values are your predictions\n",
        "prediction = dict(zip(map(str, map(int,submission_account_ids)), map(int,y_pred_submission)))\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zdlT5SRJdadv",
        "colab_type": "text"
      },
      "source": [
        "We now just send the results to our server and wait for the score!"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "E1Ev6lafdZoJ",
        "colab_type": "code",
        "outputId": "621f8fb7-833f-4ff2-8e53-f0b628dd3d26",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "# Importing stuff for http requests\n",
        "from urllib import request\n",
        "import json\n",
        "\n",
        "# We validate first that we actually send all the test accounts expected to be sent\n",
        "if y_pred_submission.shape[0] != 71683 or submission_account_ids.shape[0] != 71683:\n",
        "  raise Exception(\"You have to send all of the accounts! Expected: (71683, 71683), Got: ({}, {})\".format(y_pred_submission.shape[0], submission_account_ids.shape[0]))\n",
        "\n",
        "if \"group_name\" not in vars() or group_name == \"\":\n",
        "  group_name = input(\"Please enter your group's name:\")\n",
        "\n",
        "data = json.dumps({'submitter': group_name, 'predictions': prediction}).encode('utf-8')\n",
        "\n",
        "req = request.Request(f\"https://leaderboard.datahack.org.il/monday/api/\",\n",
        "                      headers={'Content-Type': 'application/json'},\n",
        "                      data=data)\n",
        "\n",
        "res = request.urlopen(req)\n",
        "print(json.load(res))"
      ],
      "execution_count": 352,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "{'member': 'Lamassim', 'rank': 3, 'score': 0.4496973358884644}\n"
          ],
          "name": "stdout"
        }
      ]
    }
  ]
}